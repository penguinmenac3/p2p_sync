{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App\n",
    "\n",
    "> Watches the filesystem for file changes and creates transactions based on changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definitions\n",
    "\n",
    "All imports are done at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "from typing import Dict, Any, List\n",
    "from functools import partial\n",
    "from entangle.entanglement import Entanglement\n",
    "from entangle.client import Client\n",
    "from entangle.server import listen\n",
    "\n",
    "from watchdog.observers import Observer  \n",
    "from watchdog.events import FileSystemEventHandler, DirCreatedEvent, DirDeletedEvent, FileCreatedEvent, FileDeletedEvent, DirModifiedEvent, FileModifiedEvent, DirMovedEvent, FileMovedEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_EXCLUDE_PATTERNS = [\".ipynb_checkpoints/\", \".~\", \"__pycache__/\"]\n",
    "_HANDLER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import hashlib\n",
    "def compute_md5(fname):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Save the transaction database\n",
    "def save_transactions(database_file: str, database: Dict):\n",
    "    data = json.dumps(database)\n",
    "    with open(database_file, \"w\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Load the transaction database\n",
    "def load_transactions(database_file) -> Dict:\n",
    "    if not os.path.exists(database_file):\n",
    "        print(\"No database found, creating a new one.\")\n",
    "        database = {}\n",
    "        save_transactions(database_file, database)\n",
    "\n",
    "    with open(database_file, \"r\") as f:\n",
    "        database = json.loads(f.read())\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FileChangeHandler(FileSystemEventHandler):\n",
    "    exclude_patterns = []\n",
    "    mappings = {}\n",
    "    database_path = \"database.json\"\n",
    "    \n",
    "    def get_sync_name(self, fname):\n",
    "        for namespace, path in self.mappings.items():\n",
    "            if not path.endswith(\"/\"):\n",
    "                path += \"/\"\n",
    "            if fname.startswith(path):\n",
    "                return fname.replace(path, namespace + \":\")\n",
    "\n",
    "    def on_moved(self, event):\n",
    "        \"\"\"\n",
    "        event.is_directory\n",
    "            True | False\n",
    "        event.src_path\n",
    "            path/to/observed/file\n",
    "        \"\"\"\n",
    "        raw_name = event.src_path.replace(\"\\\\\", \"/\")\n",
    "        if event.src_path.endswith(\"Neues Textdokument.txt\"):\n",
    "            self.on_created(FileCreatedEvent(event.dest_path))\n",
    "        else:\n",
    "            if not isinstance(event, DirMovedEvent):\n",
    "                self.on_deleted(FileDeletedEvent(raw_name))\n",
    "                self.on_created(FileCreatedEvent(event.dest_path))\n",
    "        \n",
    "        #if self.is_excluded(event):\n",
    "        #    return\n",
    "        #\n",
    "        #fname = self.get_sync_name(raw_name)\n",
    "        #fname_moved = self.get_sync_name(event.dest_path)\n",
    "        #\n",
    "        #transactions = load_transactions(self.database_path)\n",
    "        #transaction = {\"timestamp\": time.time(), \"type\": \"moved\", \"new_location\": fname_moved}\n",
    "        #transactions[fname] = transaction\n",
    "        #transaction = {\"timestamp\": time.time(), \"type\": \"moved\", \"old_location\": fname, \"md5\": compute_md5(event.dest_path)}\n",
    "        #transactions[fname] = transaction\n",
    "        #save_transactions(self.database_path, transactions)\n",
    "        \n",
    "        #if len(fname) > 128:\n",
    "        #    fname = fname[:63] + \"...\" + fname[-62:]            \n",
    "        #print(\"\\rMoved: {:<128} (len watches: {})\".format(fname, len(transactions)), end=\"\")\n",
    "\n",
    "\n",
    "    def on_created(self, event):\n",
    "        if event.is_directory:\n",
    "            return\n",
    "        if self.is_excluded(event):\n",
    "            return\n",
    "        \n",
    "        raw_name = event.src_path.replace(\"\\\\\", \"/\")\n",
    "        fname = self.get_sync_name(raw_name)\n",
    "        \n",
    "        transactions = load_transactions(self.database_path)\n",
    "        md5 = compute_md5(raw_name)\n",
    "        if fname not in transactions or transactions[fname][\"md5\"] != md5:\n",
    "            transaction = {\"timestamp\": time.time(), \"type\": \"created\", \"md5\": md5}\n",
    "            transactions[fname] = transaction\n",
    "            save_transactions(self.database_path, transactions)\n",
    "        \n",
    "            if len(fname) > 128:\n",
    "                fname = fname[:63] + \"...\" + fname[-62:]            \n",
    "            print(\"\\rCreated: {:<128} (len watches: {})\".format(fname, len(transactions)), end=\"\")\n",
    "\n",
    "    def on_deleted(self, event):\n",
    "        if self.is_excluded(event):\n",
    "            return\n",
    "        \n",
    "        raw_name = event.src_path.replace(\"\\\\\", \"/\")\n",
    "        fname = self.get_sync_name(raw_name)\n",
    "        \n",
    "        transactions = load_transactions(self.database_path)\n",
    "        if fname not in transactions or transactions[fname][\"type\"] != \"deleted\":\n",
    "            transaction = {\"timestamp\": time.time(), \"type\": \"deleted\"}\n",
    "            transactions[fname] = transaction\n",
    "            save_transactions(self.database_path, transactions)\n",
    "        \n",
    "            if len(fname) > 128:\n",
    "                fname = fname[:63] + \"...\" + fname[-62:]\n",
    "            print(\"\\rDeleted: {:<128} (len watches: {})\".format(fname, len(transactions)), end=\"\")\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        if event.is_directory:\n",
    "            return\n",
    "        if self.is_excluded(event):\n",
    "            return\n",
    "        \n",
    "        raw_name = event.src_path.replace(\"\\\\\", \"/\")\n",
    "        fname = self.get_sync_name(raw_name)\n",
    "        \n",
    "        transactions = load_transactions(self.database_path)\n",
    "        md5 = compute_md5(raw_name)\n",
    "        if fname not in transactions or transactions[fname][\"md5\"] != md5:\n",
    "            transaction = {\"timestamp\": time.time(), \"type\": \"modified\", \"md5\": md5}\n",
    "            transactions[fname] = transaction\n",
    "            save_transactions(self.database_path, transactions)\n",
    "\n",
    "            if len(fname) > 128:\n",
    "                fname = fname[:63] + \"...\" + fname[-62:]\n",
    "            print(\"\\rModified: {:<128} (len watches: {})\".format(fname, len(transactions)), end=\"\")\n",
    "\n",
    "    def is_excluded(self, event):\n",
    "        raw_name = event.src_path.replace(\"\\\\\", \"/\")\n",
    "        if raw_name.endswith(\"Neues Textdokument.txt\"):\n",
    "            return True\n",
    "\n",
    "        # Filter by exclude pattern.\n",
    "        for pattern in self.exclude_patterns:\n",
    "            pattern = pattern.replace(\"\\\\\", \"/\")\n",
    "            if pattern.endswith(\"/\"):\n",
    "                if pattern in raw_name:\n",
    "                    return True\n",
    "                if event.is_directory and raw_name.endswith(pattern[:-1]):\n",
    "                    return True\n",
    "            else:\n",
    "                if raw_name.split(\"/\")[-1].startswith(pattern):\n",
    "                    return True\n",
    "                if raw_name.endswith(pattern):\n",
    "                    return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def initial_scan(handler):\n",
    "    tracked_files = []\n",
    "    paths = list(handler.mappings.values())\n",
    "    \n",
    "    # check filelist for deletions or modifications\n",
    "    print(\"\\n\\rScanning Tracked Files\")\n",
    "    transactions = load_transactions(handler.database_path)\n",
    "    for fname, v in transactions.items():\n",
    "        namespace, name = fname.split(\":\")\n",
    "        disk_name = os.path.join(handler.mappings[namespace], name)\n",
    "        if len(fname) > 128:\n",
    "            fname = fname[:63] + \"...\" + fname[-62:]\n",
    "        print(\"\\rScanning: {:<128} (len queue: {})\".format(fname, len(transactions)), end=\"\")\n",
    "        \n",
    "        if os.path.exists(disk_name):\n",
    "            tracked_files.append(disk_name)\n",
    "            if compute_md5(disk_name) != v[\"md5\"]:\n",
    "                handler.on_modified(FileModifiedEvent(disk_name))\n",
    "        elif not v[\"type\"] == \"deleted\":\n",
    "            handler.on_deleted(FileDeletedEvent(disk_name))\n",
    "    print(\"\\n\\rScanning Completed\")\n",
    "\n",
    "    for path in paths:\n",
    "        print(\"n\\rScanning: {}\".format(path))\n",
    "        print(\"(no changes)\", end=\"\")\n",
    "        # check if a file was created that is not yet in filelist\n",
    "        for f in [os.path.join(root, name) for root, dirs, files in os.walk(path) for name in files]:\n",
    "            f = f.replace(\"/\", os.sep)\n",
    "            if f not in tracked_files:\n",
    "                handler.on_created(FileCreatedEvent(f))\n",
    "        print(\"\\n\\rScanning Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def on_retrieve_file(state, entanglement, data: Dict):\n",
    "    namespace, name = data[\"fname\"].split(\":\")\n",
    "    disk_name = os.path.join(state[\"handler\"].mappings[namespace], name).replace(\"/\", os.sep)\n",
    "    transactions = load_transactions(state[\"handler\"].database_path)\n",
    "    transactions[data[\"fname\"]] = data[\"transaction\"]\n",
    "    save_transactions(state[\"handler\"].database_path, transactions)\n",
    "    \n",
    "    if not data[\"transaction\"][\"type\"] == \"deleted\":\n",
    "        print(\"Writing: {}\".format(disk_name))\n",
    "        if not os.path.exists(os.path.dirname(disk_name)):\n",
    "            os.makedirs(os.path.dirname(disk_name))\n",
    "        with open(disk_name, \"wb\") as f:\n",
    "            f.write(base64.decodestring(data[\"data\"].encode(\"ascii\")))\n",
    "    else:\n",
    "        print(\"Deleting: {}\".format(disk_name))\n",
    "        if os.path.exists(disk_name):\n",
    "            os.remove(disk_name)\n",
    "\n",
    "    state[\"open_tasks\"] -= 1\n",
    "\n",
    "def retrieve_file(state, entanglement, fname):\n",
    "    data = {}\n",
    "    \n",
    "    namespace, name = fname.split(\":\")\n",
    "    disk_name = os.path.join(state[\"handler\"].mappings[namespace], name).replace(\"/\", os.sep)\n",
    "    \n",
    "    transactions = load_transactions(state[\"handler\"].database_path)\n",
    "    data[\"transaction\"] = transactions[fname]\n",
    "    data[\"fname\"] = fname\n",
    "    \n",
    "    if not data[\"transaction\"][\"type\"] == \"deleted\":\n",
    "        with open(disk_name, \"rb\") as f:\n",
    "            data[\"data\"] = base64.encodestring(f.read()).decode(\"ascii\")\n",
    "    entanglement.remote_fun(\"on_sync_retrieve_file\")(data)\n",
    "\n",
    "def on_get_database(state, entanglement, transactions: Dict):\n",
    "    transactions_local = load_transactions(state[\"handler\"].database_path)\n",
    "    \n",
    "    for key in transactions:\n",
    "        namespace, name = key.split(\":\")\n",
    "        if namespace not in state[\"handler\"].mappings:\n",
    "            continue\n",
    "        if key not in transactions_local:\n",
    "            state[\"open_tasks\"] += 1\n",
    "            entanglement.remote_fun(\"sync_retrieve_file\")(key)\n",
    "        elif key in transactions:\n",
    "            local_time = transactions_local[key][\"timestamp\"]\n",
    "            remote_time = transactions[key][\"timestamp\"]\n",
    "            if remote_time > local_time:\n",
    "                state[\"open_tasks\"] += 1\n",
    "                entanglement.remote_fun(\"sync_retrieve_file\")(key)\n",
    "    \n",
    "    state[\"open_tasks\"] -= 1\n",
    "\n",
    "def get_database(state, entanglement):\n",
    "    #print(\"get_database\")\n",
    "    transactions = load_transactions(state[\"handler\"].database_path)\n",
    "    entanglement.remote_fun(\"on_sync_get_database\")(transactions)\n",
    "    \n",
    "    \n",
    "def format_len(size):\n",
    "    if size > 1e12:\n",
    "        return \"{:.1f} TB\".format(size/1e12)\n",
    "    elif size > 1e9:\n",
    "        return \"{:.1f} GB\".format(size/1e9)\n",
    "    elif size > 1e6:\n",
    "        return \"{:.1f} MB\".format(size/1e6)\n",
    "    elif size > 1e3:\n",
    "        return \"{:.1f} KB\".format(size/1e3)\n",
    "    else:\n",
    "        return \"{:.1f} B\".format(size)\n",
    "\n",
    "def on_entangle(entanglement):\n",
    "    state = {}\n",
    "    state[\"handler\"] = _HANDLER\n",
    "    entanglement.on_sync_retrieve_file = partial(on_retrieve_file, state, entanglement)\n",
    "    entanglement.on_sync_get_database = partial(on_get_database, state, entanglement)\n",
    "    entanglement.sync_get_database = partial(get_database, state, entanglement)\n",
    "    entanglement.sync_retrieve_file = partial(retrieve_file, state, entanglement)\n",
    "    print(\"Waiting 5 seconds for readiness.\")\n",
    "    time.sleep(5)\n",
    "    print(\"Connected. Syncing...\")\n",
    "    while True:\n",
    "        #print(\"Issuing update of local database...\")\n",
    "        state[\"open_tasks\"] = 1\n",
    "        entanglement.remote_fun(\"sync_get_database\")()\n",
    "        while state[\"open_tasks\"] > 0:\n",
    "            time.sleep(1)\n",
    "\n",
    "        #print(\"Waiting 5 seconds before next sync round.\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_sync():\n",
    "    global _HANDLER\n",
    "    # Load user_data\n",
    "    if \"AppData\" in os.environ: # Windows\n",
    "        config_file = os.path.join(os.environ[\"AppData\"], \"p2p_sync\", \"config.json\")\n",
    "        syncignore_path = os.path.join(os.environ[\"AppData\"], \"p2p_sync\", \".syncignore\")\n",
    "        database_path = os.path.join(os.environ[\"AppData\"], \"p2p_sync\", \"database.json\")\n",
    "    else: # Linux\n",
    "        config_file = os.path.join(\"/home\", os.environ[\"USER\"], \".p2p_sync\", \"config.json\")\n",
    "        syncignore_path = os.path.join(\"/home\", os.environ[\"USER\"], \".p2p_sync\", \".syncignore\")\n",
    "        database_path = os.path.join(\"/home\", os.environ[\"USER\"], \".p2p_sync\", \"database.json\")\n",
    "    if not os.path.exists(config_file):\n",
    "        raise RuntimeError(\"Config does not exist: {}\".format(config_file))\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = json.loads(f.read())\n",
    "\n",
    "    # Load exclude patterns\n",
    "    exclude_patterns=_EXCLUDE_PATTERNS\n",
    "    if os.path.exists(syncignore_path):\n",
    "        with open(syncignore_path, \"r\") as f:\n",
    "            exclude_patterns = f.readlines()\n",
    "        exclude_patterns = [pattern.replace(\"\\n\", \"\") for pattern in exclude_patterns]\n",
    "        exclude_patterns = [pattern for pattern in exclude_patterns if pattern != \"\" and not pattern.startswith(\"#\")]\n",
    "        print(\"Ignore Patterns: {}\".format(exclude_patterns))\n",
    "    observer = Observer()\n",
    "    handler = FileChangeHandler()\n",
    "    handler.exclude_patterns = exclude_patterns\n",
    "    handler.database_path = database_path\n",
    "    handler.mappings = config[\"sync_to_local_folder\"]\n",
    "    initial_scan(handler)\n",
    "    _HANDLER = handler\n",
    "    for path in handler.mappings.values():\n",
    "        observer.schedule(handler, path=path, recursive=True)\n",
    "    observer.start()\n",
    "\n",
    "    print(\"Connecting...\")\n",
    "    # 1. Try connecting to all known hosts\n",
    "    clients = []\n",
    "    for hosts in config[\"known_hosts\"]:\n",
    "        clients.append(Client(host=hosts[\"host\"], port=hosts[\"port\"], password=hosts[\"password\"], user=hosts[\"user\"], callback=on_entangle, blocking=False, run_reactor=False))\n",
    "    # 2. Start own server\n",
    "    listen(host=config[\"host\"], port=config[\"port\"], callback=on_entangle, users=config[\"users\"])\n",
    "    \n",
    "    observer.stop()\n",
    "\n",
    "    observer.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "With all implemented it is time to test the implementations.\n",
    "First check what is in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore Patterns: ['.git/', '__pycache__/', '.pyc', '.egg-info/', '.ipynb_checkpoints/', '.~']\n",
      "\n",
      "Scanning Tracked Files\n",
      "Scanning: projects:recipes/ulrike/Zitronenkuchen.md                                                                                        (len queue: 189)9)\n",
      "Scanning Completed\n",
      "Scanning: /home/fuerst/Projects\n",
      "(no changes)\n",
      "Scanning Completed\n",
      "Connecting...\n",
      "WARN: NO SSL!\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "if __name__ == \"__main__\":\n",
    "    run_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
